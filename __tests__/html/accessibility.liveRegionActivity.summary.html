<!DOCTYPE html>
<html lang="en-US">
  <head>
    <link href="/assets/index.css" rel="stylesheet" type="text/css" />
    <script crossorigin="anonymous" src="/test-harness.js"></script>
    <script crossorigin="anonymous" src="/test-page-object.js"></script>
    <script crossorigin="anonymous" src="/__dist__/webchat-es5.js"></script>
  </head>
  <body>
    <div id="webchat"></div>
    <script>
      function allTextContents(element) {
        const nodes = [].slice.call(element.childNodes);
        const results = [];

        while (nodes.length) {
          const node = nodes.shift();

          if (node.nodeType === Node.TEXT_NODE) {
            results.push(node.textContent);
          } else {
            nodes.unshift(...[].slice.call(node.childNodes));
          }
        }

        return results;
      }

      run(async function () {
        const timestamp = new Date(2000, 0, 1, 12, 34, 56, 789).toISOString();
        const baseActivity = {
          attachments: [
            {
              content: 'Hello.',
              contentType: 'text/plain'
            },
            {
              content: 'World!',
              contentType: 'text/plain'
            }
          ],
          from: {
            id: 'bot',
            role: 'bot'
          },
          textFormat: 'markdown',
          timestamp,
          type: 'message'
        };

        WebChat.renderWebChat(
          {
            directLine: testHelpers.createDirectLineWithTranscript([
              {
                ...baseActivity,
                text: 'No "speak" or "summary" properties.'
              },
              {
                ...baseActivity,
                speak: 'Only "speak" property.',
                text: 'Only "speak" property.'
              },
              {
                ...baseActivity,
                summary: '2 documents.',
                text: 'Only "summary" property.'
              },
              {
                ...baseActivity,
                speak: 'Both "speak" and "summary" properties.',
                summary: 'This should not be narrated.',
                text: 'Both "speak" and "summary" properties.'
              },
              {
                ...baseActivity,
                speak: '',
                text: 'Empty string for "speak" property.'
              }
            ]),
            store: testHelpers.createStore(),
            styleOptions: {
              internalLiveRegionFadeAfter: 60000
            }
          },
          document.getElementById('webchat')
        );

        await pageConditions.uiConnected();

        const screenReaderTexts = [].map.call(
          document.querySelector('[aria-roledescription="transcript"][role="log"]').children,
          child => allTextContents(child).join('\n')
        );

        // Verify compliance of https://github.com/microsoft/botframework-sdk/blob/main/specs/botframework-activity/botframework-activity.md#summary.

        // The last activity have "speak" property set to empty string. It is treated as role="presentation" and not emitted DOM element for screen reader.
        expect(screenReaderTexts).toHaveLength(4);

        // In screen reader transcript, the attachment are rendered and narrated.
        expect(screenReaderTexts[0]).toBe(
          'Bot said:\nNo "speak" or "summary" properties.\nA text: Hello.\nA text: World!\nSent at January 1 at 12:34 PM'
        );

        expect(screenReaderTexts[1]).toBe('Bot said:\nOnly "speak" property.\nSent at January 1 at 12:34 PM');
        expect(screenReaderTexts[2]).toBe(
          'Bot said:\nOnly "summary" property.\n2 documents.\nSent at January 1 at 12:34 PM'
        );

        expect(screenReaderTexts[3]).toBe(
          'Bot said:\nBoth "speak" and "summary" properties.\nSent at January 1 at 12:34 PM'
        );

        const activityAlts = [].map.call(
          document.querySelectorAll(
            '.webchat__basic-transcript__activity .webchat__screen-reader-activity[aria-roledescription="message"]'
          ),
          element => allTextContents(element).join('\n')
        );

        // The last activity have "speak" property set to empty string. It is treated as role="presentation" and not emitted DOM element for screen reader.
        expect(screenReaderTexts).toHaveLength(4);

        // In interactive transcript, this is narrated as "2 attachments".
        expect(activityAlts[0]).toBe(
          'Bot said:\nNo "speak" or "summary" properties.\n2 attachments.\nSent at January 1 at 12:34 PM'
        );

        expect(activityAlts[1]).toBe('Bot said:\nOnly "speak" property.\nSent at January 1 at 12:34 PM');
        expect(activityAlts[2]).toBe('Bot said:\nOnly "summary" property.\n2 documents.\nSent at January 1 at 12:34 PM');
        expect(activityAlts[3]).toBe('Bot said:\nBoth "speak" and "summary" properties.\nSent at January 1 at 12:34 PM');
      });
    </script>
  </body>
</html>
