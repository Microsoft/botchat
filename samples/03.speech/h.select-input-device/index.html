<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Web Chat: Cognitive Services Speech Services using JavaScript</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!--
      For simplicity and code clarity, we are using Babel and React from unpkg.com.
    -->
    <script crossorigin="anonymous" src="https://unpkg.com/@babel/standalone@7.8.7/babel.min.js"></script>
    <script crossorigin="anonymous" src="https://unpkg.com/react@16.8.6/umd/react.development.js"></script>
    <script crossorigin="anonymous" src="https://unpkg.com/react-dom@16.8.6/umd/react-dom.development.js"></script>
    <!-- Cognitive Services Speech Services adapter is only available in full bundle -->
    <!--
      This CDN points to the latest official release of Web Chat. If you need to test against Web Chat's latest bits, please refer to pointing to Web Chat's MyGet feed:
      https://github.com/microsoft/BotFramework-WebChat#how-to-test-with-web-chats-latest-bits
    -->
    <!-- <script crossorigin="anonymous" src="https://cdn.botframework.com/botframework-webchat/latest/webchat.js"></script> -->
    <script crossorigin="anonymous" src="/webchat.js"></script>
    <style>
      html,
      body {
        height: 100%;
      }

      body {
        margin: 0;
      }

      #app {
        display: flex;
        flex-direction: column;
        height: 100%;
        width: 100%;
      }

      #app .webchat {
        overflow: hidden;
      }

      #app .app__audio-input-devices__header {
        background-color: #090;
        border-radius: 0 0 0 4px;
        color: White;
        font-size: 11px;
        padding: 0px 4px 2px;
        position: absolute;
        right: 0;
        top: 0;
      }

      #app .app__audio-input-devices {
        background-color: #ffc;
        border-radius: 4px;
        border: solid 1px rgba(0, 0, 0, 0.1);
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        flex-shrink: 0;
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans',
          'Helvetica Neue', sans-serif;
        font-size: 14px;
        margin: 10px;
        overflow: hidden;
        padding: 10px;
        position: relative;
      }

      #app .app__audio-input-devices__list {
        list-style-type: none;
        margin: 0;
        padding: 0;
      }

      #app .app__audio-input-devices__list-item:not(:first-child) {
        padding-top: 0.5em;
      }

      #app .app__audio-input-devices__device {
        display: flex;
      }

      #app .app__audio-input-devices__radio-button {
        margin-left: 0;
        margin-right: 0.5em;
      }
    </style>
  </head>
  <body>
    <div id="app" role="main"></div>
    <script type="text/babel" data-presets="es2015,react,stage-3">
      // Create a function to fetch the Cognitive Services Speech Services credentials.
      // The async function created will hold expiration information about the token and will return cached token when possible.
      function createFetchSpeechServicesCredentials() {
        let expireAfter = 0;
        let lastPromise;

        return () => {
          const now = Date.now();

          // Fetch a new token if the existing one is expiring.
          // The following article mentioned the token is only valid for 10 minutes.
          // We will invalidate the token after 5 minutes.
          // https://docs.microsoft.com/en-us/azure/cognitive-services/authentication#authenticate-with-an-authentication-token
          if (now > expireAfter) {
            expireAfter = now + 300000;
            lastPromise = fetch('https://webchat-mockbot.azurewebsites.net/speechservices/token', {
              method: 'POST'
            }).then(
              res => res.json(),
              err => {
                expireAfter = 0;

                return Promise.reject(err);
              }
            );
          }

          return lastPromise;
        };
      }

      (async function() {
        // In this demo, we are using Direct Line token from MockBot.
        // Your client code must provide either a secret or a token to talk to your bot.
        // Tokens are more secure. To learn about the differences between secrets and tokens.
        // and to understand the risks associated with using secrets, visit https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-direct-line-3-0-authentication?view=azure-bot-service-4.0

        const res = await fetch('https://webchat-mockbot.azurewebsites.net/directline/token', { method: 'POST' });
        const { token } = await res.json();

        const { createCognitiveServicesSpeechServicesPonyfillFactory, createDirectLine, ReactWebChat } = window.WebChat;
        const { useCallback, useEffect, useMemo, useState } = window.React;

        const App = () => {
          const [audioInputDevices, setAudioInputDevices] = useState([]);
          const [selectedAudioInputDeviceId, setSelectedAudioInputDeviceId] = useState('default');
          const directLine = useMemo(() => createDirectLine({ token }), [token]);
          const fetchSpeechServicesCredentials = useMemo(() => createFetchSpeechServicesCredentials(), []);

          // Create the ponyfill factory function, which can be called to create a concrete implementation of the ponyfill.
          const webSpeechPonyfillFactory = useMemo(() =>
            createCognitiveServicesSpeechServicesPonyfillFactory(
              {
                // We are passing the Promise function to the "credentials" field.
                // This function will be called every time the token is being used.
                audioInputDeviceId: selectedAudioInputDeviceId,
                credentials: fetchSpeechServicesCredentials
              },
              [selectedAudioInputDeviceId, fetchSpeechServicesCredentials]
            )
          );

          const handleSelectedAudioInputDeviceIdChange = useCallback(({ target: { value } }) =>
            setSelectedAudioInputDeviceId(value)
          );

          useEffect(() => {
            (async function() {
              setAudioInputDevices(
                (await navigator.mediaDevices.enumerateDevices()).filter(({ kind }) => kind === 'audioinput')
              );
            })();
          }, [setAudioInputDevices]);

          return (
            <React.Fragment>
              <div className="app__audio-input-devices" role="radiogroup">
                <div className="app__audio-input-devices__header">Audio input devices</div>
                <ul className="app__audio-input-devices__list">
                  {audioInputDevices.map(({ deviceId, label }) => (
                    <li className="app__audio-input-devices__list-item" key={deviceId}>
                      <label className="app__audio-input-devices__device">
                        <input
                          checked={selectedAudioInputDeviceId === deviceId}
                          className="app__audio-input-devices__radio-button"
                          name="audio-input-device-id"
                          onChange={handleSelectedAudioInputDeviceIdChange}
                          type="radio"
                          value={deviceId}
                        />
                        {label || deviceId}
                      </label>
                    </li>
                  ))}
                </ul>
              </div>
              <ReactWebChat
                className="webchat"
                directLine={directLine}
                webSpeechPonyfillFactory={webSpeechPonyfillFactory}
              />
            </React.Fragment>
          );
        };

        // Pass a Web Speech ponyfill factory to renderWebChat.
        // You can also use your own speech engine given it is compliant to W3C Web Speech API: https://w3c.github.io/speech-api/.
        // For implementor, look at createBrowserWebSpeechPonyfill.js for details.
        window.ReactDOM.render(<App />, document.getElementById('app'));
      })().catch(err => console.error(err));
    </script>
  </body>
</html>
